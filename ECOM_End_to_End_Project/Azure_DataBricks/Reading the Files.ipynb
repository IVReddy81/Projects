{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc8a7666-64cb-482d-b72d-42e629cffc32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's tell our computer the names and secrets to open our toy box in the cloud\n",
    "adls_account_name = \"liveecommstorage\"  # The name of our big toy box in the cloud\n",
    "adls_container_name = \"ecomm\"  # The special room inside our toy box\n",
    "client_id = \"2520c30b-2c3c-49fb-8249-718bc8adc701\"  # Our secret name badge to get in\n",
    "tenant_id = \"76261ceb-b88c-4d42-aaec-58d6fc10a49a\"  # The big house where our toy box lives\n",
    "client_secret = \"lpm8Q~HRLWzkeCvj_n1wYvowyWUGz1QBK5NIUa.M\"  # Our super secret password\n",
    "\n",
    "# Now, let's tell Spark how to use our secret keys to open the toy box\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.auth.type.{adls_account_name}.dfs.core.windows.net\", \"OAuth\"\n",
    ")  # Tell Spark to use a magic key (OAuth) to open the box\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth.provider.type.{adls_account_name}.dfs.core.windows.net\",\n",
    "    \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    ")  # Tell Spark what kind of magic key to use\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth2.client.id.{adls_account_name}.dfs.core.windows.net\",\n",
    "    client_id,\n",
    ")  # Give Spark our secret name badge\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth2.client.secret.{adls_account_name}.dfs.core.windows.net\",\n",
    "    client_secret,\n",
    ")  # Give Spark our super secret password\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth2.client.endpoint.{adls_account_name}.dfs.core.windows.net\",\n",
    "    f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\",\n",
    ")  # Tell Spark where to go to check our badge and password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f54c864-fb26-402a-bc78-399a3ccbd77c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Reading Customer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "821994a5-d35b-4715-b616-2b8aa8e58ad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's read our customer data from a CSV file in the cloud and call it df_customer\n",
    "df_customer = (\n",
    "    spark.read.format(\"csv\")  # Tell Spark we want to read a CSV file (like a table)\n",
    "    .options(header=True, inferSchema=True)  # The first row has names, and Spark guesses the type of each column\n",
    "    .load(\n",
    "        f\"abfss://{adls_container_name}@{adls_account_name}.dfs.core.windows.net/bronze/olist_customers_dataset.csv\"\n",
    "    )  # This is the path to our customer file in the cloud toy box\n",
    ")\n",
    "\n",
    "# Show our customer data in a pretty table so we can look at it\n",
    "display(df_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249f160c-e91d-403a-a4e2-65cd28128978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make a path to the folder where our files live in the cloud\n",
    "base_path = (\n",
    "    f\"abfss://{adls_container_name}@{adls_account_name}.dfs.core.windows.net/bronze/\"\n",
    ")\n",
    "\n",
    "# These are the names of our CSV files (like picture books, but with data)\n",
    "item = \"olist_order_items_dataset.csv\"\n",
    "geolocation = \"olist_geolocation_dataset.csv\"\n",
    "review = \"olist_order_reviews_dataset.csv\"\n",
    "oders = \"olist_orders_dataset.csv\"\n",
    "products = \"olist_products_dataset.csv\"\n",
    "sellers = \"olist_sellers_dataset.csv\"\n",
    "\n",
    "# Read the order items file and put it in a table called df_item\n",
    "df_item = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=True, inferSchema=True)\n",
    "    .load(f\"{base_path}{item}\")\n",
    ")\n",
    "\n",
    "# Read the geolocation file and put it in a table called df_geolocation\n",
    "df_geolocation = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=True, inferSchema=True)\n",
    "    .load(f\"{base_path}{geolocation}\")\n",
    ")\n",
    "\n",
    "# Read the reviews file and put it in a table called df_review\n",
    "df_review = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=True, inferSchema=True)\n",
    "    .load(f\"{base_path}{review}\")\n",
    ")\n",
    "\n",
    "# Read the orders file and put it in a table called df_orders\n",
    "df_orders = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=True, inferSchema=True)\n",
    "    .load(f\"{base_path}{oders}\")\n",
    ")\n",
    "\n",
    "# Read the products file and put it in a table called df_products\n",
    "df_products = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=True, inferSchema=True)\n",
    "    .load(f\"{base_path}{products}\")\n",
    ")\n",
    "\n",
    "# Read the sellers file and put it in a table called df_sellers\n",
    "df_sellers = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=True, inferSchema=True)\n",
    "    .load(f\"{base_path}{sellers}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec131a7-8195-4d77-a2aa-d77adc8026e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Installing PyMongo\n",
    "    1. Go to Compute\n",
    "    2. Click on your compute \n",
    "    3. click on libraries\n",
    "    4. Click on \"Install New\"\n",
    "    5. select \"PyPi\" for python libraries\n",
    "    6. select pymongo\n",
    "    7. finally select install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62aa0096-5bf9-46d2-a8e8-2864478053fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0089770d-64f5-4b93-bd43-c025ea8943a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing module\n",
    "from pymongo import MongoClient\n",
    "\n",
    "hostname = \"gdgskp.h.filess.io\"\n",
    "database = \"mongodb_fireblank\"\n",
    "port = \"27018\"\n",
    "username = \"mongodb_fireblank\"\n",
    "password = \"dcbde0733a2f7c1ad344fc780e0e5385416cb210\"\n",
    "\n",
    "uri = \"mongodb://\" + username + \":\" + password + \"@\" + hostname + \":\" + port + \"/\" + database\n",
    "\n",
    "# Connect with the portnumber and host\n",
    "client = MongoClient(uri)\n",
    "\n",
    "# Access database\n",
    "mydatabase = client[database]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8374c9-d874-4c4d-a764-b8d73bdae791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access the 'product_categories' collection from the MongoDB database\n",
    "collection = mydatabase[\"product_categories\"]\n",
    "\n",
    "# Retrieve all documents from the collection and convert them to a list\n",
    "data = list(collection.find())\n",
    "\n",
    "# Import the pandas library for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Create a pandas DataFrame from the list of MongoDB documents\n",
    "mongo_df = pd.DataFrame(data)\n",
    "\n",
    "# Remove the '_id' column from the DataFrame, as it is not needed\n",
    "mongo_df.drop(\"_id\", axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame for a quick preview\n",
    "mongo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dd1a394-41c3-4343-b513-855bd567e177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame 'mongo_df' (containing MongoDB data) to a Spark DataFrame\n",
    "df_catagories = spark.createDataFrame(mongo_df)\n",
    "\n",
    "# Display the Spark DataFrame in a rich tabular format in Databricks\n",
    "display(df_catagories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba8edaf-d944-42b6-8a40-fe8a1e252297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Connecting to MySql to get payments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a16d2d-2cf0-4f5e-b935-6bd5e0ec34a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The tool is called 'mysql-connector-python' and it helps us talk to a MySQL database.\n",
    "%pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "759e8409-2c00-4272-84fe-9245eecc11d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mysql.connector  # This helps us talk to a MySQL database\n",
    "import pandas as pd  # This helps us work with tables of data\n",
    "\n",
    "hostname = \"2wh2ue.h.filess.io\"  # The address where our database lives\n",
    "database = \"mysql_signeager\"  # The name of our database\n",
    "port = \"3307\"  # The door number we use to enter the database\n",
    "username = \"mysql_signeager\"  # Our name to log in to the database\n",
    "password = \"ac86df6155972e5f57f6f6c5b7b7ed842df3833c\"  # Our secret code to log in\n",
    "\n",
    "try:\n",
    "    # We try to connect to the database using the address, name, door number, our name, and secret code\n",
    "    connection = mysql.connector.connect(host=hostname, database=database, user=username, password=password, port=port)\n",
    "    if connection.is_connected():  # If we successfully connect\n",
    "        query = \"SELECT * FROM olist_order_payments\"  # We want to get all the data from the 'olist_order_payments' table\n",
    "        df_payments = pd.read_sql(query, connection)  # We put that data into a table we can work with\n",
    "        display(df_payments)  # We show the table so we can see it\n",
    "\n",
    "except Error as e:  # If something goes wrong\n",
    "    print(\"Error while connecting to MySQL\", e)  # We print out what went wrong\n",
    "\n",
    "finally:\n",
    "    if connection.is_connected():  # If we are still connected to the database\n",
    "        connection.close()  # We close the connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121885f5-e13e-458a-bc0e-6004020244cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Turn pandas table into Spark table so Spark can play with it\n",
    "df_payments = spark.createDataFrame(df_payments)\n",
    "\n",
    "# Show the Spark table so we can see it\n",
    "display(df_payments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5cfeff6-df75-483b-b165-cfebfb9e3567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Orders transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26973421-b4c1-4d5d-b019-b1f3e85322ee",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752381901601}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_orders.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85779907-32c1-4532-88f8-20d0ba21ae60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Add new columns to df_orders DataFrame with various date differences and a delay label\n",
    "df_orders = (\n",
    "    df_orders.withColumn(\n",
    "        \"order_purchase_to_approve\",\n",
    "        datediff(df_orders.order_approved_at, df_orders.order_purchase_timestamp),\n",
    "    )  # Days between purchase and approval\n",
    "    .withColumn(\n",
    "        \"order_purchase_to_carrier_delivery\",\n",
    "        datediff(\n",
    "            df_orders.order_delivered_carrier_date, df_orders.order_purchase_timestamp\n",
    "        ),\n",
    "    )  # Days between purchase and carrier delivery\n",
    "    .withColumn(\n",
    "        \"order_purchase_to_estimated_deliery\",\n",
    "        datediff(\n",
    "            df_orders.order_estimated_delivery_date, df_orders.order_purchase_timestamp\n",
    "        ),\n",
    "    )  # Days between purchase and estimated delivery\n",
    "    .withColumn(\n",
    "        \"order_purchase_to_customer_delivery\",\n",
    "        datediff(\n",
    "            df_orders.order_delivered_customer_date, df_orders.order_purchase_timestamp\n",
    "        ),\n",
    "    )  # Days between purchase and customer delivery\n",
    "    .withColumn(\n",
    "        \"delay\",\n",
    "        when(col(\"order_purchase_to_estimated_deliery\") > 0, \"Good\").otherwise(\n",
    "            \"Bad\"\n",
    "        ),\n",
    "    )  # Label as 'Good' if estimated delivery is after purchase, else 'Bad'\n",
    ")\n",
    "\n",
    "display(df_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88412ca6-b824-42ca-ab21-e0e8b6fe142f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Joins\n",
    "![](/Workspace/Users/ivreddyivr@outlook.com/Venkat_Projects/ECOM_End_to_End_Project/Azure_DataBricks/ecomm_joins.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fbbe453-61dd-49da-8f4d-88caef7ea415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = (\n",
    "    df_orders.join(\n",
    "        df_customer, \"customer_id\", \"inner\"\n",
    "    )  # Join df_orders with df_customer on customer_id using inner join\n",
    "    .join(\n",
    "        df_payments, \"order_id\", \"inner\"\n",
    "    )  # Join the result with df_payments on order_id using inner join\n",
    "    .join(\n",
    "        df_item, \"order_id\", \"right\"\n",
    "    )  # Join the result with df_item on order_id using right join\n",
    "    .join(\n",
    "        df_products, \"product_id\", \"inner\"\n",
    "    )  # Join the result with df_products on product_id using inner join\n",
    "    .join(\n",
    "        df_sellers, \"seller_id\", \"right\"\n",
    "    )  # Join the result with df_sellers on seller_id using right join\n",
    "    .join(  # Join the result with df_geolocation on matching zip code prefixes using inner join\n",
    "        df_geolocation,\n",
    "        df_geolocation.geolocation_zip_code_prefix == df_sellers.seller_zip_code_prefix,\n",
    "        \"inner\",\n",
    "    )\n",
    ")\n",
    "\n",
    "display(final_df.head(5))  # Display the first 5 rows of the final DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36cd4d8b-c724-4f6f-966b-36979b28d0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Removing duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "141c69f6-9ea2-4864-a3e5-0b9e7e0f04ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code identifies duplicate column names in the final_df DataFrame.\n",
    "# It iterates through all column names, tracks seen columns, and collects duplicates in need_to_delete.\n",
    "\n",
    "cols = final_df.columns\n",
    "seen_cols = set()\n",
    "need_to_delete = []\n",
    "for i in cols:\n",
    "    if i in seen_cols:\n",
    "        need_to_delete.append(i)\n",
    "    else:\n",
    "        seen_cols.add(i)\n",
    "\n",
    "print(need_to_delete)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "373886e9-834d-43f1-b513-75c9729d7f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Writing data to silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af47b14-f201-4b50-abf9-eff135e63935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.repartition(1).write.format(\"delta\").save(\n",
    "    \"abfss://ecomm@liveecommstorage.dfs.core.windows.net/silver/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443f09e9-55c3-45c8-8ec2-31d8d8c353b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eadaba8-1aa6-4199-bbc1-92e4e69001be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Reading the Files",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
