{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98745af1-4ba3-42a9-aa83-9aae48b9a982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"prjvenkat\"  # Azure storage account name\n",
    "container_name = \"walmart\"  # Azure container name\n",
    "client_id = \"68bcc65c-52c5-4ab7-82e0-797cc8c70fef\"  # Azure AD application client ID\n",
    "tenant_id = \"76261ceb-b88c-4d42-aaec-58d6fc10a49a\"  # Azure AD tenant ID\n",
    "client_secret = \"Dva8Q~fC3W3WzfGUatFqbgW2~8mVnXKQEPVlxaaE\"  # Azure AD application client secret\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\"\n",
    ")  # Set authentication type to OAuth for the storage account\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\",\n",
    "    \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    ")  # Set OAuth provider type for the storage account\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\",\n",
    "    client_id,\n",
    ")  # Set client ID for OAuth authentication\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\",\n",
    "    client_secret,\n",
    ")  # Set client secret for OAuth authentication\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\",\n",
    "    f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\",\n",
    ")  # Set OAuth2 token endpoint using the tenant ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52af8f3f-2c28-4856-98b5-9d9ecef7235c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/bronze\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9783c965-ab1b-43e1-aa7d-4b75fbfa3660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Reading feature.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13467591-e6a1-4608-ba67-a44c430a5ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the features.csv file from the bronze folder in Azure Data Lake as a Spark DataFrame\n",
    "df_feature = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=True, inferSchema=True)  # Read header and infer schema\n",
    "    .load(\n",
    "        f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/bronze/features.csv\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the DataFrame in Databricks\n",
    "display(df_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99b1e4b4-c017-4289-a4e5-1358ecd6efb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing values in specified columns with 0.00\n",
    "df_feature = df_feature.fillna(\n",
    "    {\n",
    "        \"Temperature\": 0.00,   # Replace nulls in Temperature with 0.00\n",
    "        \"Fuel_Price\": 0.00,    # Replace nulls in Fuel_Price with 0.00\n",
    "        \"MarkDown1\": 0.00,     # Replace nulls in MarkDown1 with 0.00\n",
    "        \"MarkDown2\": 0.00,     # Replace nulls in MarkDown2 with 0.00\n",
    "        \"MarkDown3\": 0.00,     # Replace nulls in MarkDown3 with 0.00\n",
    "        \"MarkDown4\": 0.00,     # Replace nulls in MarkDown4 with 0.00\n",
    "        \"MarkDown5\": 0.00,     # Replace nulls in MarkDown5 with 0.00\n",
    "        \"CPI\": 0.00,           # Replace nulls in CPI with 0.00\n",
    "        \"Unemployment\": 0.00,  # Replace nulls in Unemployment with 0.00\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "display(df_feature.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5676835e-7de7-43a9-9840-56ea778ba97e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *  # Import all functions from pyspark.sql.functions\n",
    "\n",
    "df_feature = (\n",
    "    df_feature.withColumn(\n",
    "        \"Temperature\",\n",
    "        when(col(\"Temperature\") == \"NA\", 0.00).otherwise(col(\"Temperature\")),  # Replace 'NA' in Temperature with 0.00\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Fuel_Price\", when(col(\"Fuel_Price\") == \"NA\", 0.00).otherwise(col(\"Fuel_Price\"))  # Replace 'NA' in Fuel_Price with 0.00\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"MarkDown1\", when(col(\"MarkDown1\") == \"NA\", 0.00).otherwise(col(\"MarkDown1\"))  # Replace 'NA' in MarkDown1 with 0.00\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"MarkDown2\", when(col(\"MarkDown2\") == \"NA\", 0.00).otherwise(col(\"MarkDown2\"))  # Replace 'NA' in MarkDown2 with 0.00\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"MarkDown3\", when(col(\"MarkDown3\") == \"NA\", 0.00).otherwise(col(\"MarkDown3\"))  # Replace 'NA' in MarkDown3 with 0.00\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"MarkDown4\", when(col(\"MarkDown4\") == \"NA\", 0.00).otherwise(col(\"MarkDown4\"))  # Replace 'NA' in MarkDown4 with 0.00\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"MarkDown5\", when(col(\"MarkDown5\") == \"NA\", 0.00).otherwise(col(\"MarkDown5\"))  # Replace 'NA' in MarkDown5 with 0.00\n",
    "    )\n",
    "    .withColumn(\"CPI\", when(col(\"CPI\") == \"NA\", 0.00).otherwise(col(\"CPI\")))  # Replace 'NA' in CPI with 0.00\n",
    "    .withColumn(\n",
    "        \"Unemployment\",\n",
    "        when(col(\"Unemployment\") == \"NA\", 0.00).otherwise(col(\"Unemployment\")),  # Replace 'NA' in Unemployment with 0.00\n",
    "    )\n",
    "    .withColumn(\"IsHoliday\", when(col(\"IsHoliday\") == \"False\", 0).otherwise(1))  # Convert 'IsHoliday' from 'False' to 0, else 1\n",
    ")\n",
    "display(df_feature.head(5))  # Display the first 5 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b7475da-5aca-48c0-9b29-3a95b5cd63d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_feature.write.format(\"parquet\").save(\n",
    "    \"abfss://walmart@prjvenkat.dfs.core.windows.net/silver/features.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfaba6bd-7d03-413a-8e88-80518cd8694f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Reading stores.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11644c56-4171-462a-b629-e58722900622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the stores.csv file from the bronze folder in Azure Data Lake as a Spark DataFrame\n",
    "df_stores = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=True, inferSchema=True)  # Read header and infer schema\n",
    "    .load(\n",
    "        f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/bronze/stores.csv\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "display(df_stores.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f17a57c-5910-464a-be5c-1918ca052592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of rows in df_stores where the 'Size' column is null\n",
    "df_stores.filter(col(\"Size\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf19a1b6-6b31-4784-a986-b3b7151a0f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of rows in df_stores where the 'Type' column is null\n",
    "df_stores.filter(col(\"Type\").isNull()).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d6d73e-dc9f-42af-aaa4-eb82b3a7256d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the cleaned df_stores DataFrame to Azure Data Lake in Parquet format\n",
    "\n",
    "df_stores.write.format(\"parquet\").save(\n",
    "    \"abfss://walmart@prjvenkat.dfs.core.windows.net/silver/stores.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d8b57d0-2e5a-4c26-b7fe-74d3d593f19c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Reading test.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce749c34-0227-45f2-ae41-0220391c10f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the test.csv file from Azure Data Lake (bronze layer) as a Spark DataFrame\n",
    "df_test = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=True, inferSchema=True)\n",
    "    .load(\n",
    "        f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/bronze/test.csv\"\n",
    "    )\n",
    ")\n",
    "display(df_test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2856d44-c6db-4cc5-9910-61e412989519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test.where(col(\"Dept\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e5c4d8-e7b9-4159-a397-1b8fb722dd81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test=df_test.withColumn(\"IsHoliday\",when(col(\"IsHoliday\")==\"false\", 0).otherwise(1))\n",
    "df_test.show(2,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b63566a-08b5-43dd-a9c3-592da084f015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the cleaned df_test DataFrame to Azure Data Lake in Parquet format\n",
    "df_test.write.format(\"parquet\").save(\n",
    "    \"abfss://walmart@prjvenkat.dfs.core.windows.net/silver/test.parquet\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4911b1a-2e8d-4b35-9ef3-71589c51c210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Reading train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb83bcd6-b4a1-45cb-a485-328e7a1b4107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the training data from Azure Data Lake in CSV format\n",
    "df_train = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=True, inferSchema=True)\n",
    "    .load(\n",
    "        f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/bronze/train.csv\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the first 5 rows of the training DataFrame\n",
    "display(df_train.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec094e6f-a140-4a8b-ac2d-76a0b9f0e395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train.filter(col(\"Weekly_Sales\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b68fcc-12ef-42a5-87b7-3125b8f4613e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import all functions from pyspark.sql.functions\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Convert the 'IsHoliday' column from string to integer (0 for 'false', 1 otherwise)\n",
    "df_train = df_train.withColumn(\n",
    "    \"IsHoliday\", when(df_train.IsHoliday == \"false\", 0).otherwise(1)\n",
    ")\n",
    "\n",
    "# Display the first 2 rows of the updated DataFrame with full column content and row information\n",
    "display(df_train.limit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00a01d1-6bfe-4d1b-b4e2-6c0e6825edae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train.write.parquet(\n",
    "    \"abfss://walmart@prjvenkat.dfs.core.windows.net/silver/train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4a4bce-9637-4991-a505-9ec132662724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"abfss://walmart@prjvenkat.dfs.core.windows.net/silver\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe9521f-5486-4bf5-b861-a5c10dd341cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Move the 'train.parquet' directory to 'train' within the silver layer in Azure Data Lake\n",
    "dbutils.fs.mv(\n",
    "    \"abfss://walmart@prjvenkat.dfs.core.windows.net/silver/train.parquet\",\n",
    "    \"abfss://walmart@prjvenkat.dfs.core.windows.net/silver/train\",\n",
    "    recurse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52f718e-249b-4908-86db-5a16c06e8af4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"abfss://walmart@prjvenkat.dfs.core.windows.net/silver\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22cd1b7-7242-48fc-98b8-b3fcace60bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove the 'train' directory from the silver layer in Azure Data Lake\n",
    "dbutils.fs.rm(\n",
    "    f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/silver/train\",\n",
    "    True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b57126bb-6cc8-4146-9eba-1d77b1868ab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Move the 'stores.parquet' directory to 'stores' within the silver layer in Azure Data Lake\n",
    "dbutils.fs.mv(\n",
    "    f\"abfss://walmart@prjvenkat.dfs.core.windows.net/silver/stores.parquet\",\n",
    "    f\"abfss://walmart@prjvenkat.dfs.core.windows.net/silver/stores\",\n",
    "    True,\n",
    ")\n",
    "\n",
    "# Move the 'features.parquet' directory to 'features' within the silver layer in Azure Data Lake\n",
    "dbutils.fs.mv(\n",
    "    f\"abfss://walmart@prjvenkat.dfs.core.windows.net/silver/features.parquet\",\n",
    "    f\"abfss://walmart@prjvenkat.dfs.core.windows.net/silver/features\",\n",
    "    True,\n",
    ")\n",
    "\n",
    "# Move the 'test.parquet' directory to 'test' within the silver layer in Azure Data Lake\n",
    "dbutils.fs.mv(\n",
    "    f\"abfss://walmart@prjvenkat.dfs.core.windows.net/silver/test.parquet\",\n",
    "    f\"abfss://walmart@prjvenkat.dfs.core.windows.net/silver/test\",\n",
    "    True,\n",
    ")\n",
    "\n",
    "# Display the contents of the 'silver' layer to verify the moves\n",
    "display(dbutils.fs.ls(\"abfss://walmart@prjvenkat.dfs.core.windows.net/silver\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze to Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
